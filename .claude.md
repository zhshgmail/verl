# VERL NPU Training - Session Context

**Last Updated**: 2025-12-29 14:30 CST
**Branch**: `feature/npu-aqn-test`

## CRITICAL BUG FOUND AND FIXED

### Bug: vLLM RMSNorm Detection Failure
**All previous AQN tests (Test 1, Test 2, Test 3 first run) had NO noise applied!**

The code used `isinstance(module, Qwen2RMSNorm)` with transformers classes, but vLLM uses its own model implementations with different class names. The fix detects RMSNorm by class name:
```python
def _is_rmsnorm(module):
    class_name = module.__class__.__name__.lower()
    return 'rmsnorm' in class_name and hasattr(module, 'weight')
```

## Current Session: Test 3 v2 - WITH REAL NOISE

### ✅ FIX CONFIRMED WORKING (14:30 CST)
```
[AQN] Applied noise to 57 RMSNorm layers (skipped 0 excluded layers)
```

### Active Tests (Started 14:18 CST)
- **Test 3a v2** (verl-a3cloud): QeRL params (sigma 0.05→0.0005), log: `/tmp/test3a_qerl_v2.log`
- **Test 3b v2** (verl-a3cloud-2): Strong AQN (sigma 0.1→0.001), log: `/tmp/test3b_strong_v2.log`

### Hardware
- Host: A3Cloud (7.150.12.17)
- Containers: `verl-a3cloud` (Test 3a), `verl-a3cloud-2` (Test 3b)
- NPUs: 8x Ascend 910C per container (64GB HBM each)

### Test 3 Parameters
```yaml
# Common
data.train_batch_size: 128
actor_rollout_ref.rollout.gpu_memory_utilization: 0.8
actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu: 4

# Test 3a (QeRL original)
trainer.noise_injection.sigma_start: 0.05
trainer.noise_injection.sigma_end: 0.0005
trainer.noise_injection.num_stages: 10

# Test 3b (2x stronger)
trainer.noise_injection.sigma_start: 0.1
trainer.noise_injection.sigma_end: 0.001
trainer.noise_injection.num_stages: 10
```

### Current Results
| Step | Test 3a (QeRL) | Test 3b (Strong) |
|------|----------------|------------------|
| 0 | 24.11% | 23.73% |
| 20 | (pending) | (pending) |
| ... | | |

### Issues Fixed This Session

1. **Data path**: `/root/data/gsm8k` → `/tmp/train_gsm8k_v4.parquet`
2. **ppo_micro_batch_size**: 16 → 4 (20 not divisible by 16)
3. **FRACTAL_NZ error**: Added `export VLLM_ASCEND_ENABLE_NZ=0`
4. **MoE detection false positive**:
   - Qwen2.5's `mlp.gate_proj` was triggering "gate" detection
   - Fixed: `['experts', 'shared_expert', 'num_experts', 'expert_']`
5. **Default target_modules**:
   - vllm_rollout.py had hardcoded `['post_attention_layernorm']`
   - ppo_trainer.yaml had same default
   - Fixed: Both now use `[]` (empty) for auto-detection

### Key Code Changes

1. **`verl/utils/noise_injection.py`**
   - Improved `_detect_moe_model()` to avoid gate_proj false positive
   - Dense: ALL RMSNorm, MoE: post_attention_layernorm only

2. **`verl/workers/rollout/vllm_rollout/vllm_rollout.py`**
   - Changed default target_modules/exclude_patterns to None
   - Enables auto-detection based on model type

3. **`verl/trainer/config/ppo_trainer.yaml`**
   - Changed noise_injection.target_modules from `["post_attention_layernorm"]` to `[]`
   - Changed noise_injection.exclude_patterns from `["input_layernorm"]` to `[]`

### Check Commands (v2 logs)

```bash
# Check Test 3a v2 status
ssh root@7.150.12.17 "docker exec verl-a3cloud grep -E 'Training Progress|val-core' /tmp/test3a_qerl_v2.log | tail -5"

# Check Test 3b v2 status
ssh root@7.150.12.17 "docker exec verl-a3cloud-2 grep -E 'Training Progress|val-core' /tmp/test3b_strong_v2.log | tail -5"

# CRITICAL: Verify noise is being applied (should show non-zero layers)
ssh root@7.150.12.17 "docker exec verl-a3cloud grep 'Applied noise to' /tmp/test3a_qerl_v2.log | tail -3"
ssh root@7.150.12.17 "docker exec verl-a3cloud-2 grep 'Applied noise to' /tmp/test3b_strong_v2.log | tail -3"

# Get all validation results
ssh root@7.150.12.17 "docker exec verl-a3cloud grep 'val-core/openai/gsm8k/acc' /tmp/test3a_qerl_v2.log"
ssh root@7.150.12.17 "docker exec verl-a3cloud-2 grep 'val-core/openai/gsm8k/acc' /tmp/test3b_strong_v2.log"
```

### Research Hypothesis: HW Heterogeneous Error Robustness

**Original QeRL**: Noise → Robustness to Quantization Error (GPU→quantized GPU)
**Our Hypothesis**: Noise → Robustness to HW Heterogeneous Error (GPU→NPU)

Test 3 v2 validates this hypothesis:
- BF16 (not quantized) training on NPU
- Base model from GPU, post-training on NPU
- If AQN helps, it's a novel application beyond QeRL's original scope

Sources of HW heterogeneous error:
- BF16 rounding differences, MatMul accumulation order
- Activation function precision (tanh, gelu, silu)
- Tensor core vs NPU cube unit behaviors

### Previous Test Results (INVALIDATED - No noise applied due to bug)
| Step | Baseline | "AQN" (actually no noise) | Delta |
|------|----------|---------------------------|-------|
| 0 | 23.96% | 23.35% | -0.61% |
| 20 | 74.60% | 73.31% | -1.29% |
| ... | ... | ... | ... |

**Note**: All previous "AQN" tests were actually baselines due to the vLLM RMSNorm detection bug.

### Files Modified
- `verl/utils/noise_injection.py`
- `verl/workers/rollout/vllm_rollout/vllm_rollout.py`
- `verl/trainer/config/ppo_trainer.yaml`
- `run_test3a_qerl.sh` (local)
- `run_test3b_strong.sh` (local)

### TODO
1. Monitor tests until completion (~2 hours from 13:37)
2. Collect validation results at steps 20, 40, 60, 80, 100, 116
3. Update TEMP_TEST3_SESSION_20251229.md with results
4. Commit all changes after tests complete

---
**Detailed session log**: See `TEMP_TEST3_SESSION_20251229.md`
