# Claude Code Session State

**Last Updated**: 2025-12-26 09:35 UTC
**Session**: NPU Training - Ray Rank Visibility Issue
**Status**: User going to sleep, agent continuing work

---

## CRITICAL ISSUE: Ray Only Sees 12/16 NPUs

**Hardware**: 8 physical NPUs × 2 chips = **16 logical ranks**
**Torch reports**: 16 (`torch_npu.device_count() = 16`)
**Ray reports**: **12.0 NPU** (WRONG!)

**Impact**:
- Can't use all resources
- **Likely caused Qwen3-30B-A3B hang** (I only gave it 8 ranks when it needs 16)
- Training fails with "available 12 < desired 16"

---

## Root Cause Revelation

### What I Thought
"Large MoE models (128 experts) hang on NPU due to scale issues"

### What Actually Happened
I only configured **8 ranks** when the model needs **16 ranks**!

**Evidence**:
- Qwen3-30B-A3B: Configured TP=4, EP=2 = 8 workers on n_gpus_per_node=8
- Ray actors: 5-7/8 spawned, hung waiting for more
- **Never tried with 16 ranks because I didn't know about 910C architecture!**

---

## User Instructions

**Priority**: Fix Ray to see all 16 ranks, then retry Qwen3-30B-A3B

**Steps**:
1. Read temp docs (TEMP_DO_NOT_COMMIT_RAY_RANK_ISSUE.md)
2. Read docs/qerl/ (NPU_LESSONS_LEARNED.md, NPU_ARCHITECTURE_CORRECTION.md)
3. Make plan to fix Ray
4. Execute plan
5. Spawn QA agent to verify
6. If QA approves, move to next step
7. Before context limit, update temp docs

**If need to use 2 nodes**:
- Simulate: Node 1 (NPU 0-7), Node 2 (NPU 8-15)
- Each node: 8 ranks
- Config: `n_gpus_per_node=8, nnodes=2`

---

## Key Files

### Documentation
1. **`TEMP_DO_NOT_COMMIT_RAY_RANK_ISSUE.md`** - Ray problem analysis & solutions
2. **`docs/qerl/NPU_LESSONS_LEARNED.md`** - My mistakes documented
3. **`docs/qerl/NPU_ARCHITECTURE_CORRECTION.md`** - 910C architecture facts

### Configs
1. **`qwen15_moe_gsm8k_grpo_npu.yaml`** - Small MoE (60 experts)
   - Current: TP=4, EP=4, needs 16 ranks
   - Can downgrade to TP=4, EP=3 for 12 ranks if needed

2. **Need to create**: `qwen3_30b_a3b_npu_16ranks.yaml`
   - Large MoE (128 experts)
   - TP=4, EP=4 = 16 ranks
   - Enable offloading (param, optimizer, grad)

### Data
- **`/data/nfs/data/gsm8k/train_chat.parquet`** - 7,473 samples, ready
- **`/data/nfs/data/gsm8k/test_chat.parquet`** - 1,319 samples, ready
- **`convert_gsm8k_to_chat.py`** - Converter script

---

## 910C Architecture (VERIFIED)

```
Physical: 8 NPUs (NPU 0-7)
Logical:  16 ranks (2 chips per physical NPU)
Memory:   64GB per rank × 16 = 1024GB total

torch_npu.device_count() = 16 ✓
npu-smi info shows 8 NPUs, each with "Chip Count: 2" ✓
```

**For VERL config**: Use `n_gpus_per_node` based on logical ranks (16)

---

## What Went Wrong (My Mistakes)

1. **Session 1**: Configured 4 NPUs (didn't know there were 8)
2. **Session 2**: Configured 8 NPUs (didn't know about 2 chips each = 16 ranks)
3. **Session 3**: Configured 12 NPUs (saw Ray report, didn't question it)
4. **NOW**: Understand it's 16 ranks, but Ray only sees 12

**All documented in**: `docs/qerl/NPU_LESSONS_LEARNED.md`

---

## Goals

### PRIMARY: Qwen3-30B-A3B with R3+AQN
- Model: Qwen3-30B-A3B-Instruct-2507 (128 experts, 30B params)
- Dataset: GSM8K math problems
- Algorithm: GRPO
- Features: R3 (disabled for now), AQN noise injection
- **Needs**: 16 ranks to work properly

### SECONDARY: Qwen1.5-MoE-A2.7B
- Smaller MoE for testing (60 experts, 2.7B params)
- Can work with 12 ranks if needed
- Good for validating approach

---

## Commands Quick Reference

### Check Ray Status
```bash
ssh root@7.150.12.17 "docker exec verl-a3cloud ray status"
```

### Fix Ray (Try First)
```bash
ssh root@7.150.12.17 "docker exec verl-a3cloud ray stop --force"
ssh root@7.150.12.17 "docker exec verl-a3cloud ray start --head --num-gpus=16 --resources='{\"NPU\": 16}'"
ssh root@7.150.12.17 "docker exec verl-a3cloud ray status"  # Verify shows 16
```

### Launch Training
```bash
ssh root@7.150.12.17 "tmux new-session -d -s training \
  'docker exec -i verl-a3cloud bash -c \
  \"cd /verl && python3 -m verl.trainer.main_ppo \
  --config-path /tmp --config-name <CONFIG_NAME> \
  2>&1 | tee /tmp/training.log\"'"

# Monitor
ssh root@7.150.12.17 "docker exec verl-a3cloud tail -f /tmp/training.log"
```

---

## Environment

- **Host**: root@7.150.12.17 (A3Cloud)
- **Container**: verl-a3cloud (official image)
- **VERL**: v0.7.0.dev0 (commit d82d39d)
- **vLLM-Ascend**: 0.11.0rc1
- **MindSpeed**: v0.12.1
- **CANN**: 8.3.RC1

---

## Next Steps Priority

1. **FIX RAY** to see 16 NPUs (highest priority)
2. **TEST Qwen3-30B-A3B** with 16 ranks (the real goal)
3. If Qwen3 works, add AQN and test
4. Document findings for publication

---

**Status**: Ray visibility issue blocks progress. Must fix before training.
**User**: Sleeping, agent continuing autonomous work.
